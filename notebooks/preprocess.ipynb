{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    A function for loading csv data into dataframe df.\n",
    "    '''\n",
    "\n",
    "    #Location of csv file\n",
    "    csv_file = '../raw_data/air_pollution_data.csv'\n",
    "\n",
    "    #Loading csv file into df dataframe\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    A function to clean raw data:\n",
    "    - Dropping unuseful columns\n",
    "    - Dropping rows with year = NA\n",
    "    - Dropping rows where pm10_concentration AND pm25_concentration AND no2_concentration are NA\n",
    "    '''\n",
    "\n",
    "    #Dropping columns: web_link, reference, iso3, who_ms, population_source, version, pm10_tempcov, pm25_tempcov, no2_tempcov\n",
    "    df.drop(columns=['web_link',\n",
    "                     'reference',\n",
    "                     'iso3',\n",
    "                     'who_ms',\n",
    "                     'population_source',\n",
    "                     'version',\n",
    "                     'pm10_tempcov',\n",
    "                     'pm25_tempcov',\n",
    "                     'no2_tempcov'],\n",
    "            inplace=True)\n",
    "\n",
    "    #Dropping rows where year is NA (3 rows for India)\n",
    "    df.dropna(subset=['year'], inplace=True)\n",
    "\n",
    "    #Dropping rows where pm10_concentration AND pm25_concentration AND no2_concentration are NA\n",
    "    df.dropna(how='all', subset=['pm10_concentration', 'pm25_concentration', 'no2_concentration'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>who_region</th>\n",
       "      <th>iso3</th>\n",
       "      <th>country_name</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>version</th>\n",
       "      <th>pm10_concentration</th>\n",
       "      <th>pm25_concentration</th>\n",
       "      <th>no2_concentration</th>\n",
       "      <th>pm10_tempcov</th>\n",
       "      <th>pm25_tempcov</th>\n",
       "      <th>no2_tempcov</th>\n",
       "      <th>type_of_stations</th>\n",
       "      <th>reference</th>\n",
       "      <th>web_link</th>\n",
       "      <th>population</th>\n",
       "      <th>population_source</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>who_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>V4.0 (2018), V4.0 (2018), V4.0 (2018), V4.0 (2...</td>\n",
       "      <td>23.238</td>\n",
       "      <td>11.491</td>\n",
       "      <td>28.841</td>\n",
       "      <td>87.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Urban, Urban, Suburban</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>246146.0</td>\n",
       "      <td>manual, manual, manual, manual</td>\n",
       "      <td>43.367900</td>\n",
       "      <td>-8.418571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>V6.0  (2023), V6.0  (2023), V6.0  (2023)</td>\n",
       "      <td>27.476</td>\n",
       "      <td>15.878</td>\n",
       "      <td>19.575</td>\n",
       "      <td>96.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Urban, Urban, Suburban</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247604.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.368033</td>\n",
       "      <td>-8.418233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>V6.0  (2023), V6.0  (2023), V6.0  (2023), V6.0...</td>\n",
       "      <td>25.515</td>\n",
       "      <td>14.004</td>\n",
       "      <td>22.731</td>\n",
       "      <td>98.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247604.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>V6.0  (2023), V6.0  (2023), V6.0  (2023), V6.0...</td>\n",
       "      <td>23.057</td>\n",
       "      <td>13.160</td>\n",
       "      <td>20.204</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247604.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>ESP</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>V6.0  (2023), V6.0  (2023), V6.0  (2023), V6.0...</td>\n",
       "      <td>26.849</td>\n",
       "      <td>14.114</td>\n",
       "      <td>21.543</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247604.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  who_region iso3 country_name          city    year  \\\n",
       "0      4_Eur  ESP        Spain  A Coruna/ESP  2013.0   \n",
       "1      4_Eur  ESP        Spain  A Coruna/ESP  2014.0   \n",
       "2      4_Eur  ESP        Spain  A Coruna/ESP  2015.0   \n",
       "3      4_Eur  ESP        Spain  A Coruna/ESP  2016.0   \n",
       "4      4_Eur  ESP        Spain  A Coruna/ESP  2017.0   \n",
       "\n",
       "                                             version  pm10_concentration  \\\n",
       "0  V4.0 (2018), V4.0 (2018), V4.0 (2018), V4.0 (2...              23.238   \n",
       "1           V6.0  (2023), V6.0  (2023), V6.0  (2023)              27.476   \n",
       "2  V6.0  (2023), V6.0  (2023), V6.0  (2023), V6.0...              25.515   \n",
       "3  V6.0  (2023), V6.0  (2023), V6.0  (2023), V6.0...              23.057   \n",
       "4  V6.0  (2023), V6.0  (2023), V6.0  (2023), V6.0...              26.849   \n",
       "\n",
       "   pm25_concentration  no2_concentration  pm10_tempcov  pm25_tempcov  \\\n",
       "0              11.491             28.841          87.0          46.0   \n",
       "1              15.878             19.575          96.0          88.0   \n",
       "2              14.004             22.731          98.0          71.0   \n",
       "3              13.160             20.204          98.0          98.0   \n",
       "4              14.114             21.543          97.0          97.0   \n",
       "\n",
       "   no2_tempcov                  type_of_stations reference web_link  \\\n",
       "0         93.0            Urban, Urban, Suburban       NaN      NaN   \n",
       "1         95.0            Urban, Urban, Suburban       NaN      NaN   \n",
       "2         98.0  Urban, Urban, Suburban, Suburban       NaN      NaN   \n",
       "3         98.0  Urban, Urban, Suburban, Suburban       NaN      NaN   \n",
       "4         98.0  Urban, Urban, Suburban, Suburban       NaN      NaN   \n",
       "\n",
       "   population               population_source   latitude  longitude  who_ms  \n",
       "0    246146.0  manual, manual, manual, manual  43.367900  -8.418571       1  \n",
       "1    247604.0                             NaN  43.368033  -8.418233       1  \n",
       "2    247604.0                             NaN  43.370375  -8.422900       1  \n",
       "3    247604.0                             NaN  43.370375  -8.422900       1  \n",
       "4    247604.0                             NaN  43.370375  -8.422900       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_concentrations(df):\n",
    "    '''\n",
    "    A function that classifies the concentrations of NO2, PM10, and PM2.5 into categories based on the European Air Quality Index (AQI) classification.\n",
    "    '''\n",
    "    # Define classification limits\n",
    "    no2_limits = [0, 40, 90, 120, 230, 340, 1000]\n",
    "    pm10_limits = [0, 10, 20, 25, 50, 75, 800]\n",
    "    pm25_limits = [0, 20, 40, 50, 100, 150, 1200]\n",
    "\n",
    "    # Classify PM10 concentrations\n",
    "    df['pm10_class'] = pd.cut(df['pm10_concentration'], bins=pm10_limits, labels=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "    # Classify PM2.5 concentrations\n",
    "    df['pm25_class'] = pd.cut(df['pm25_concentration'], bins=pm25_limits, labels=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "    # Classify NO2 concentrations\n",
    "    df['no2_class'] = pd.cut(df['no2_concentration'], bins=no2_limits, labels=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "    # Determine the target class as the maximum of the three pollutant classes\n",
    "    df['target_class'] = df[['no2_class', 'pm10_class', 'pm25_class']].max(axis=1)\n",
    "\n",
    "    # Drop the class concentration columns\n",
    "    df = df.drop(columns=['pm10_class', 'no2_class', 'pm25_class'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = classify_concentrations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>who_region</th>\n",
       "      <th>country_name</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>pm10_concentration</th>\n",
       "      <th>pm25_concentration</th>\n",
       "      <th>no2_concentration</th>\n",
       "      <th>type_of_stations</th>\n",
       "      <th>population</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>23.238</td>\n",
       "      <td>11.491</td>\n",
       "      <td>28.841</td>\n",
       "      <td>Urban, Urban, Suburban</td>\n",
       "      <td>246146.0</td>\n",
       "      <td>43.367900</td>\n",
       "      <td>-8.418571</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>27.476</td>\n",
       "      <td>15.878</td>\n",
       "      <td>19.575</td>\n",
       "      <td>Urban, Urban, Suburban</td>\n",
       "      <td>247604.0</td>\n",
       "      <td>43.368033</td>\n",
       "      <td>-8.418233</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>25.515</td>\n",
       "      <td>14.004</td>\n",
       "      <td>22.731</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>247604.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>23.057</td>\n",
       "      <td>13.160</td>\n",
       "      <td>20.204</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>247604.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna/ESP</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>26.849</td>\n",
       "      <td>14.114</td>\n",
       "      <td>21.543</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>247604.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  who_region country_name          city    year  pm10_concentration  \\\n",
       "0      4_Eur        Spain  A Coruna/ESP  2013.0              23.238   \n",
       "1      4_Eur        Spain  A Coruna/ESP  2014.0              27.476   \n",
       "2      4_Eur        Spain  A Coruna/ESP  2015.0              25.515   \n",
       "3      4_Eur        Spain  A Coruna/ESP  2016.0              23.057   \n",
       "4      4_Eur        Spain  A Coruna/ESP  2017.0              26.849   \n",
       "\n",
       "   pm25_concentration  no2_concentration                  type_of_stations  \\\n",
       "0              11.491             28.841            Urban, Urban, Suburban   \n",
       "1              15.878             19.575            Urban, Urban, Suburban   \n",
       "2              14.004             22.731  Urban, Urban, Suburban, Suburban   \n",
       "3              13.160             20.204  Urban, Urban, Suburban, Suburban   \n",
       "4              14.114             21.543  Urban, Urban, Suburban, Suburban   \n",
       "\n",
       "   population   latitude  longitude target_class  \n",
       "0    246146.0  43.367900  -8.418571            3  \n",
       "1    247604.0  43.368033  -8.418233            4  \n",
       "2    247604.0  43.370375  -8.422900            4  \n",
       "3    247604.0  43.370375  -8.422900            3  \n",
       "4    247604.0  43.370375  -8.422900            4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Data Summary:\n",
      "                    Missing Count  Missing Percentage (%)  Unique Count\n",
      "who_region                      0                    0.00             7\n",
      "iso3                            0                    0.00           123\n",
      "country_name                    0                    0.00           124\n",
      "city                            0                    0.00          7182\n",
      "year                            3                    0.01            13\n",
      "version                         0                    0.00           380\n",
      "pm10_concentration          11426                   28.50         18052\n",
      "pm25_concentration          18368                   45.81         12647\n",
      "no2_concentration           13164                   32.83         17265\n",
      "pm10_tempcov                17695                   44.13           101\n",
      "pm25_tempcov                23508                   58.63           101\n",
      "no2_tempcov                 16696                   41.64           101\n",
      "type_of_stations            16767                   41.82           325\n",
      "reference                   33446                   83.41           185\n",
      "web_link                    38308                   95.54           145\n",
      "population                  17665                   44.05          6493\n",
      "population_source           21996                   54.86           339\n",
      "latitude                        0                    0.00         14012\n",
      "longitude                       0                    0.00         13973\n",
      "who_ms                          0                    0.00             2\n",
      "\n",
      "Number of duplicate rows in the dataset: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40098"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a comprehensive summary for the DataFrame\n",
    "\n",
    "# Number of missing values per column\n",
    "missing_count = data.isna().sum()\n",
    "\n",
    "# Percentage of missing values per column\n",
    "missing_percentage = (data.isna().mean() * 100).round(2)\n",
    "\n",
    "# Number of unique values per column\n",
    "unique_count = data.nunique()\n",
    "\n",
    "# Number of duplicate rows in the DataFrame\n",
    "duplicate_count = data.duplicated().sum()\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Missing Count': missing_count,\n",
    "    'Missing Percentage (%)': missing_percentage,\n",
    "    'Unique Count': unique_count\n",
    "})\n",
    "\n",
    "print(\"Comprehensive Data Summary:\")\n",
    "print(summary_df)\n",
    "print(f\"\\nNumber of duplicate rows in the dataset: {duplicate_count}\")\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_population(country, city):\n",
    "    try:\n",
    "        # Clean up the city name, removing country code if present\n",
    "        city_name = city.split('/')[0].strip()\n",
    "        # Construct the Open Data Soft API URL\n",
    "        url = f\"https://public.opendatasoft.com/api/records/1.0/search/?dataset=geonames-all-cities-with-a-population-500&q={city_name}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'records' in data and data['records']:\n",
    "                # Extract population from the first result\n",
    "                population = data['records'][0]['fields'].get('population', None)\n",
    "                return int(population) if population else None\n",
    "            else:\n",
    "                print(f\"No records found for {city}\")\n",
    "        else:\n",
    "            print(f\"Request failed for {city}. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {city}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Clean up the 'city' column\n",
    "#df['city'] = df['city'].apply(lambda x: x.split('/')[0].strip())\n",
    "\n",
    "# Identify rows with NaN population\n",
    "#rows_with_nan = df[df['population'].isna()]\n",
    "\n",
    "# Fetch population data only for rows with NaN population\n",
    "##for index, row in rows_with_nan.iterrows():\n",
    "#    country = row['country_name']\n",
    "#    city = row['city']\n",
    "#    population = fetch_population(country, city)\n",
    "#    if population:\n",
    "#        df.at[index, 'population'] = population\n",
    "# #       print(f\"Filled missing population for {city}, {country} with {population}\")\n",
    "# #   else:\n",
    "#        print(f\"Could not find population for {city}, {country}\")\n",
    "\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['city'] = data['city'].apply(lambda x: x.split('/')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = data.merge(df[['city', 'country_name','year', 'population']],\n",
    "                       on=['city', 'country_name','year',],\n",
    "                       how='left',\n",
    "                       suffixes=('', '_new'))\n",
    "\n",
    "# Update the 'population' column in 'data' with the 'population_new' values from 'df' where available\n",
    "merged_df['population'] = merged_df['population_new'].combine_first(merged_df['population'])\n",
    "\n",
    "# Drop the 'population_new' column after updating\n",
    "merged_df.drop(columns=['population_new'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Data Summary:\n",
      "                    Missing Count  Missing Percentage (%)  Unique Count\n",
      "who_region                      0                    0.00             7\n",
      "iso3                            0                    0.00           123\n",
      "country_name                    0                    0.00           124\n",
      "city                            0                    0.00          7136\n",
      "year                            3                    0.01            13\n",
      "version                         0                    0.00           380\n",
      "pm10_concentration          11426                   28.50         18052\n",
      "pm25_concentration          18368                   45.81         12647\n",
      "no2_concentration           13164                   32.83         17265\n",
      "pm10_tempcov                17695                   44.13           101\n",
      "pm25_tempcov                23508                   58.63           101\n",
      "no2_tempcov                 16696                   41.64           101\n",
      "type_of_stations            16767                   41.82           325\n",
      "reference                   33446                   83.41           185\n",
      "web_link                    38308                   95.54           145\n",
      "population                  17665                   44.05          6493\n",
      "population_source           21996                   54.86           339\n",
      "latitude                        0                    0.00         14012\n",
      "longitude                       0                    0.00         13973\n",
      "who_ms                          0                    0.00             2\n",
      "\n",
      "Number of duplicate rows in the dataset: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40098"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a comprehensive summary for the DataFrame\n",
    "\n",
    "# Number of missing values per column\n",
    "missing_count = merged_df.isna().sum()\n",
    "\n",
    "# Percentage of missing values per column\n",
    "missing_percentage = (merged_df.isna().mean() * 100).round(2)\n",
    "\n",
    "# Number of unique values per column\n",
    "unique_count = merged_df.nunique()\n",
    "\n",
    "# Number of duplicate rows in the DataFrame\n",
    "duplicate_count = merged_df.duplicated().sum()\n",
    "\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Missing Count': missing_count,\n",
    "    'Missing Percentage (%)': missing_percentage,\n",
    "    'Unique Count': unique_count\n",
    "})\n",
    "\n",
    "print(\"Comprehensive Data Summary:\")\n",
    "\n",
    "print(summary_df)\n",
    "print(f\"\\nNumber of duplicate rows in the dataset: {duplicate_count}\")\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('../raw_data/air_pollution_data_upd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Data Summary:\n",
      "                    Missing Count  Missing Percentage (%)  Unique Count\n",
      "who_region                      0                    0.00             7\n",
      "iso3                            0                    0.00           123\n",
      "country_name                    0                    0.00           124\n",
      "city                            0                    0.00          7136\n",
      "year                            3                    0.01            13\n",
      "version                         0                    0.00           380\n",
      "pm10_concentration          11426                   28.50         18052\n",
      "pm25_concentration          18368                   45.81         12647\n",
      "no2_concentration           13164                   32.83         17265\n",
      "pm10_tempcov                17695                   44.13           101\n",
      "pm25_tempcov                23508                   58.63           101\n",
      "no2_tempcov                 16696                   41.64           101\n",
      "type_of_stations            16767                   41.82           325\n",
      "reference                   33446                   83.41           185\n",
      "web_link                    38308                   95.54           145\n",
      "population                   1411                    3.52          8451\n",
      "population_source           21996                   54.86           339\n",
      "latitude                        0                    0.00         14012\n",
      "longitude                       0                    0.00         13973\n",
      "who_ms                          0                    0.00             2\n",
      "\n",
      "Number of duplicate rows in the dataset: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40098"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a comprehensive summary for the DataFrame\n",
    "\n",
    "# Number of missing values per column\n",
    "missing_count = data2.isna().sum()\n",
    "\n",
    "# Percentage of missing values per column\n",
    "missing_percentage = (data2.isna().mean() * 100).round(2)\n",
    "\n",
    "# Number of unique values per column\n",
    "unique_count = data2.nunique()\n",
    "\n",
    "# Number of duplicate rows in the DataFrame\n",
    "duplicate_count = data2.duplicated().sum()\n",
    "\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Missing Count': missing_count,\n",
    "    'Missing Percentage (%)': missing_percentage,\n",
    "    'Unique Count': unique_count\n",
    "})\n",
    "\n",
    "print(\"Comprehensive Data Summary:\")\n",
    "\n",
    "print(summary_df)\n",
    "print(f\"\\nNumber of duplicate rows in the dataset: {duplicate_count}\")\n",
    "len(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polution Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts in concentrations by year:\n",
      "        pm10_concentration  pm25_concentration  no2_concentration  year\n",
      "year                                                                   \n",
      "2010.0                 678                1948                707     0\n",
      "2011.0                 215                 448                500     0\n",
      "2012.0                 230                 555                644     0\n",
      "2013.0                 869                2277               1138     0\n",
      "2014.0                 756                1834               1159     0\n",
      "2015.0                1079                1858               1472     0\n",
      "2016.0                1165                1727               1250     0\n",
      "2017.0                1159                1654               1154     0\n",
      "2018.0                1879                1692               1810     0\n",
      "2019.0                1691                1633               1690     0\n",
      "2020.0                1118                1580               1008     0\n",
      "2021.0                 562                1056                519     0\n",
      "2022.0                  22                 104                110     0\n"
     ]
    }
   ],
   "source": [
    "temp = data2[['pm10_concentration','pm25_concentration','no2_concentration','year']]\n",
    "nan_counts = temp.groupby('year').apply(lambda x: x.isnull().sum())\n",
    "\n",
    "print(\"NaN counts in concentrations by year:\")\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_concentrations(df):\n",
    "    '''\n",
    "    Classifies the concentrations of NO2, PM10, and PM2.5 into categories based on the European Air Quality Index (AQI) classification.\n",
    "    Sets the target class as the maximum of the three classified pollutant concentrations.\n",
    "    '''\n",
    "    # Define classification limits\n",
    "    no2_limits = [0, 40, 90, 120, 230, 340, 1000]\n",
    "    pm10_limits = [0, 10, 20, 25, 50, 75, 800]\n",
    "    pm25_limits = [0, 20, 40, 50, 100, 150, 1200]\n",
    "\n",
    "    # Classify PM10 concentrations\n",
    "    df['pm10_class'] = pd.cut(df['pm10_concentration'], bins=pm10_limits, labels=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "    # Classify PM2.5 concentrations\n",
    "    df['pm25_class'] = pd.cut(df['pm25_concentration'], bins=pm25_limits, labels=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "    # Classify NO2 concentrations\n",
    "    df['no2_class'] = pd.cut(df['no2_concentration'], bins=no2_limits, labels=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "    # Determine the target class as the maximum of the three pollutant classes\n",
    "    df['target_class'] = df[['pm10_class', 'pm25_class', 'no2_class']].apply(lambda row: row.max(), axis=1)\n",
    "\n",
    "    # Drop the intermediate class columns\n",
    "    df = df.drop(columns=['pm10_class', 'no2_class', 'pm25_class'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = clean_data(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Data Summary:\n",
      "                    Missing Count  Missing Percentage (%)  Unique Count\n",
      "who_region                      0                    0.00             7\n",
      "country_name                    0                    0.00           124\n",
      "city                            0                    0.00          7136\n",
      "year                            0                    0.00            13\n",
      "pm10_concentration          11330                   28.32         18052\n",
      "pm25_concentration          18273                   45.68         12647\n",
      "no2_concentration           13068                   32.67         17265\n",
      "type_of_stations            16695                   41.74           325\n",
      "population                   1383                    3.46          8447\n",
      "latitude                        0                    0.00         14010\n",
      "longitude                       0                    0.00         13971\n",
      "\n",
      "Number of duplicate rows in the dataset: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40002"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a comprehensive summary for the DataFrame\n",
    "\n",
    "# Number of missing values per column\n",
    "missing_count = data2.isna().sum()\n",
    "\n",
    "# Percentage of missing values per column\n",
    "missing_percentage = (data2.isna().mean() * 100).round(2)\n",
    "\n",
    "# Number of unique values per column\n",
    "unique_count = data2.nunique()\n",
    "\n",
    "# Number of duplicate rows in the DataFrame\n",
    "duplicate_count = data2.duplicated().sum()\n",
    "\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Missing Count': missing_count,\n",
    "    'Missing Percentage (%)': missing_percentage,\n",
    "    'Unique Count': unique_count\n",
    "})\n",
    "\n",
    "print(\"Comprehensive Data Summary:\")\n",
    "\n",
    "print(summary_df)\n",
    "print(f\"\\nNumber of duplicate rows in the dataset: {duplicate_count}\")\n",
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>who_region</th>\n",
       "      <th>country_name</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>pm10_concentration</th>\n",
       "      <th>pm25_concentration</th>\n",
       "      <th>no2_concentration</th>\n",
       "      <th>type_of_stations</th>\n",
       "      <th>population</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>23.238</td>\n",
       "      <td>11.491</td>\n",
       "      <td>28.841</td>\n",
       "      <td>Urban, Urban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.367900</td>\n",
       "      <td>-8.418571</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>27.476</td>\n",
       "      <td>15.878</td>\n",
       "      <td>19.575</td>\n",
       "      <td>Urban, Urban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.368033</td>\n",
       "      <td>-8.418233</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>25.515</td>\n",
       "      <td>14.004</td>\n",
       "      <td>22.731</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>23.057</td>\n",
       "      <td>13.160</td>\n",
       "      <td>20.204</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>26.849</td>\n",
       "      <td>14.114</td>\n",
       "      <td>21.543</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40093</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>Republic of Korea</td>\n",
       "      <td>경기도</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>57.335</td>\n",
       "      <td>36.457</td>\n",
       "      <td>0.029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.337200</td>\n",
       "      <td>126.724100</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40094</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>Republic of Korea</td>\n",
       "      <td>경기도</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>50.838</td>\n",
       "      <td>31.586</td>\n",
       "      <td>0.027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.337200</td>\n",
       "      <td>126.724100</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40095</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>Republic of Korea</td>\n",
       "      <td>경기도</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>55.568</td>\n",
       "      <td>31.013</td>\n",
       "      <td>0.028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.337200</td>\n",
       "      <td>126.724100</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40096</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>China</td>\n",
       "      <td>虎英公园北</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.012778</td>\n",
       "      <td>113.794444</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40097</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>China</td>\n",
       "      <td>虎英公园北</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.012778</td>\n",
       "      <td>113.794444</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40002 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      who_region       country_name      city    year  pm10_concentration  \\\n",
       "0          4_Eur              Spain  A Coruna  2013.0              23.238   \n",
       "1          4_Eur              Spain  A Coruna  2014.0              27.476   \n",
       "2          4_Eur              Spain  A Coruna  2015.0              25.515   \n",
       "3          4_Eur              Spain  A Coruna  2016.0              23.057   \n",
       "4          4_Eur              Spain  A Coruna  2017.0              26.849   \n",
       "...          ...                ...       ...     ...                 ...   \n",
       "40093      6_Wpr  Republic of Korea       경기도  2017.0              57.335   \n",
       "40094      6_Wpr  Republic of Korea       경기도  2018.0              50.838   \n",
       "40095      6_Wpr  Republic of Korea       경기도  2019.0              55.568   \n",
       "40096      6_Wpr              China     虎英公园北  2018.0                 NaN   \n",
       "40097      6_Wpr              China     虎英公园北  2019.0                 NaN   \n",
       "\n",
       "       pm25_concentration  no2_concentration  \\\n",
       "0                  11.491             28.841   \n",
       "1                  15.878             19.575   \n",
       "2                  14.004             22.731   \n",
       "3                  13.160             20.204   \n",
       "4                  14.114             21.543   \n",
       "...                   ...                ...   \n",
       "40093              36.457              0.029   \n",
       "40094              31.586              0.027   \n",
       "40095              31.013              0.028   \n",
       "40096              30.649                NaN   \n",
       "40097              29.731                NaN   \n",
       "\n",
       "                       type_of_stations  population   latitude   longitude  \\\n",
       "0                Urban, Urban, Suburban    246056.0  43.367900   -8.418571   \n",
       "1                Urban, Urban, Suburban    246056.0  43.368033   -8.418233   \n",
       "2      Urban, Urban, Suburban, Suburban    246056.0  43.370375   -8.422900   \n",
       "3      Urban, Urban, Suburban, Suburban    246056.0  43.370375   -8.422900   \n",
       "4      Urban, Urban, Suburban, Suburban    246056.0  43.370375   -8.422900   \n",
       "...                                 ...         ...        ...         ...   \n",
       "40093                               NaN         NaN  37.337200  126.724100   \n",
       "40094                               NaN         NaN  37.337200  126.724100   \n",
       "40095                               NaN         NaN  37.337200  126.724100   \n",
       "40096                               NaN         NaN  23.012778  113.794444   \n",
       "40097                               NaN         NaN  23.012778  113.794444   \n",
       "\n",
       "       target_class  \n",
       "0                 3  \n",
       "1                 4  \n",
       "2                 4  \n",
       "3                 3  \n",
       "4                 4  \n",
       "...             ...  \n",
       "40093             5  \n",
       "40094             5  \n",
       "40095             5  \n",
       "40096             2  \n",
       "40097             2  \n",
       "\n",
       "[40002 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_complete = classify_concentrations(data2)\n",
    "data_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Data Summary:\n",
      "                    Missing Count  Missing Percentage (%)  Unique Count\n",
      "who_region                      0                    0.00             7\n",
      "country_name                    0                    0.00           124\n",
      "city                            0                    0.00          7136\n",
      "year                            0                    0.00            13\n",
      "pm10_concentration          11330                   28.32         18052\n",
      "pm25_concentration          18273                   45.68         12647\n",
      "no2_concentration           13068                   32.67         17265\n",
      "type_of_stations            16695                   41.74           325\n",
      "population                   1383                    3.46          8447\n",
      "latitude                        0                    0.00         14010\n",
      "longitude                       0                    0.00         13971\n",
      "target_class                    0                    0.00             6\n",
      "\n",
      "Number of duplicate rows in the dataset: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40002"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a comprehensive summary for the DataFrame\n",
    "\n",
    "# Number of missing values per column\n",
    "missing_count = data_complete.isna().sum()\n",
    "\n",
    "# Percentage of missing values per column\n",
    "missing_percentage = (data_complete.isna().mean() * 100).round(2)\n",
    "\n",
    "# Number of unique values per column\n",
    "unique_count = data_complete.nunique()\n",
    "\n",
    "# Number of duplicate rows in the DataFrame\n",
    "duplicate_count = data_complete.duplicated().sum()\n",
    "\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Missing Count': missing_count,\n",
    "    'Missing Percentage (%)': missing_percentage,\n",
    "    'Unique Count': unique_count\n",
    "})\n",
    "\n",
    "print(\"Comprehensive Data Summary:\")\n",
    "\n",
    "print(summary_df)\n",
    "print(f\"\\nNumber of duplicate rows in the dataset: {duplicate_count}\")\n",
    "len(data_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_stations(station_type):\n",
    "    '''\n",
    "    Simplifies the station type string by removing duplicates and sorting.\n",
    "\n",
    "    Args:\n",
    "    - station_type (str): A string containing station types separated by ', ' e.g. Urban, urban, urban.\n",
    "\n",
    "    Returns:\n",
    "    - str: Simplified station types joined into a single string e.g \"Urban, urban, urban\" returns \"Urban\"\n",
    "\n",
    "    If station_type is NaN (missing), returns 'unknown'.'''\n",
    "\n",
    "    if pd.isna(station_type):\n",
    "        return \"unknown\"\n",
    "    unique_types = sorted(set(station_type.split(', ')))\n",
    "    return ', '.join(unique_types)\n",
    "\n",
    "def simplified_station_type(df):\n",
    "    '''\n",
    "    Adds a new column 'simplified_station_type' to the DataFrame 'df' based on simplifying 'type_of_stations'.\n",
    "\n",
    "    Args:\n",
    "    - df (pandas.DataFrame): The DataFrame containing the column 'type_of_stations' to be simplified.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: The input DataFrame 'df' with an additional column 'simplified_station_type'.\n",
    "\n",
    "    This function applies the 'simplify_stations' function to each value in the 'type_of_stations' column\n",
    "    and stores the simplified result in a new column 'simplified_station_type'\n",
    "    '''\n",
    "\n",
    "    df['type_of_stations'] = df['type_of_stations'].astype('string') #converts type_of_stations column into a string in order to apply simplify_stations function\n",
    "    df['simplified_station_type'] = df['type_of_stations'].apply(simplify_stations)\n",
    "    return df\n",
    "\n",
    "def impute_stations(df):\n",
    "    '''\n",
    "    Imputes the values of missing type_of_stations based on similar pollution metrics of know types of stations using KNN imputer'''\n",
    "\n",
    "    #first simplify station names using simplified_station_type function\n",
    "    simplified_station_type(df)\n",
    "\n",
    "    # Manually map known types of stations to numerical labels from stations3 df\n",
    "    type_mapping = {\n",
    "        'Unknown': np.nan, #will need this to be nan for imputer to work\n",
    "        'Urban': 1,\n",
    "        'Rural': 2,\n",
    "        'Suburban': 3,\n",
    "        'Suburban, Urban': 4,\n",
    "        'Rural, Urban': 5,\n",
    "        'Rural, Suburban, Urban': 6,\n",
    "        'Rural, Suburban': 7,\n",
    "        'Background': 8,\n",
    "        'Residential And Commercial Area': 9,\n",
    "        'Traffic': 10,\n",
    "        'Residential And Commercial Area, Urban Traffic': 11,\n",
    "        'Background, Traffic': 12,\n",
    "        'Industrial': 13,\n",
    "        'Residential And Commercial Area, Urban Traffic': 14,\n",
    "        'Industrial, Urban': 15,\n",
    "        'Industrial, Rural, Urban': 16,\n",
    "        'Residential': 17,\n",
    "        'Fond Urbain, Traffic': 18,\n",
    "        'Residential - industrial': 19\n",
    "    }\n",
    "\n",
    "    df['encoded_station_type'] = df['simplified_station_type'].map(type_mapping) # encode simpified_station_type column to feed into KNN imputer\n",
    "\n",
    "    # Select features for imputation\n",
    "    features = ['pm10_concentration', 'pm25_concentration', 'no2_concentration', 'encoded_station_type'] #features to be learned by imputer\n",
    "\n",
    "    # Perform KNN imputation\n",
    "    imputer = KNNImputer(n_neighbors=5) #instantiate imputer\n",
    "    df_imputed = imputer.fit_transform(df[features]) #returns array with learned features\n",
    "\n",
    "    # Assign imputed values back to DataFrame\n",
    "    df['encoded_station_type_imputed'] = df_imputed[:, -1]  # Assuming encoded_station_type is the last column after imputation\n",
    "\n",
    "    # Revert encoded_station_type back to original categorical values\n",
    "    reverse_mapping = {v: k for k, v in type_mapping.items() if pd.notna(v)}  # Reverse mapping excluding NaNs. source >> https://stackoverflow.com/questions/483666/reverse-invert-a-dictionary-mapping\n",
    "\n",
    "    df['final_station_type'] = df['encoded_station_type_imputed'].round().astype(int).map(reverse_mapping).fillna(np.nan)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complete = impute_stations(data_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>who_region</th>\n",
       "      <th>country_name</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>pm10_concentration</th>\n",
       "      <th>pm25_concentration</th>\n",
       "      <th>no2_concentration</th>\n",
       "      <th>type_of_stations</th>\n",
       "      <th>population</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>target_class</th>\n",
       "      <th>simplified_station_type</th>\n",
       "      <th>encoded_station_type</th>\n",
       "      <th>encoded_station_type_imputed</th>\n",
       "      <th>final_station_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>23.238</td>\n",
       "      <td>11.491</td>\n",
       "      <td>28.841</td>\n",
       "      <td>Urban, Urban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.367900</td>\n",
       "      <td>-8.418571</td>\n",
       "      <td>3</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>27.476</td>\n",
       "      <td>15.878</td>\n",
       "      <td>19.575</td>\n",
       "      <td>Urban, Urban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.368033</td>\n",
       "      <td>-8.418233</td>\n",
       "      <td>4</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>25.515</td>\n",
       "      <td>14.004</td>\n",
       "      <td>22.731</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>4</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>23.057</td>\n",
       "      <td>13.160</td>\n",
       "      <td>20.204</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>3</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_Eur</td>\n",
       "      <td>Spain</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>26.849</td>\n",
       "      <td>14.114</td>\n",
       "      <td>21.543</td>\n",
       "      <td>Urban, Urban, Suburban, Suburban</td>\n",
       "      <td>246056.0</td>\n",
       "      <td>43.370375</td>\n",
       "      <td>-8.422900</td>\n",
       "      <td>4</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Suburban, Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40093</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>Republic of Korea</td>\n",
       "      <td>경기도</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>57.335</td>\n",
       "      <td>36.457</td>\n",
       "      <td>0.029</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.337200</td>\n",
       "      <td>126.724100</td>\n",
       "      <td>5</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40094</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>Republic of Korea</td>\n",
       "      <td>경기도</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>50.838</td>\n",
       "      <td>31.586</td>\n",
       "      <td>0.027</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.337200</td>\n",
       "      <td>126.724100</td>\n",
       "      <td>5</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40095</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>Republic of Korea</td>\n",
       "      <td>경기도</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>55.568</td>\n",
       "      <td>31.013</td>\n",
       "      <td>0.028</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.337200</td>\n",
       "      <td>126.724100</td>\n",
       "      <td>5</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40096</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>China</td>\n",
       "      <td>虎英公园北</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.012778</td>\n",
       "      <td>113.794444</td>\n",
       "      <td>2</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40097</th>\n",
       "      <td>6_Wpr</td>\n",
       "      <td>China</td>\n",
       "      <td>虎英公园北</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.012778</td>\n",
       "      <td>113.794444</td>\n",
       "      <td>2</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Rural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40002 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      who_region       country_name      city    year  pm10_concentration  \\\n",
       "0          4_Eur              Spain  A Coruna  2013.0              23.238   \n",
       "1          4_Eur              Spain  A Coruna  2014.0              27.476   \n",
       "2          4_Eur              Spain  A Coruna  2015.0              25.515   \n",
       "3          4_Eur              Spain  A Coruna  2016.0              23.057   \n",
       "4          4_Eur              Spain  A Coruna  2017.0              26.849   \n",
       "...          ...                ...       ...     ...                 ...   \n",
       "40093      6_Wpr  Republic of Korea       경기도  2017.0              57.335   \n",
       "40094      6_Wpr  Republic of Korea       경기도  2018.0              50.838   \n",
       "40095      6_Wpr  Republic of Korea       경기도  2019.0              55.568   \n",
       "40096      6_Wpr              China     虎英公园北  2018.0                 NaN   \n",
       "40097      6_Wpr              China     虎英公园北  2019.0                 NaN   \n",
       "\n",
       "       pm25_concentration  no2_concentration  \\\n",
       "0                  11.491             28.841   \n",
       "1                  15.878             19.575   \n",
       "2                  14.004             22.731   \n",
       "3                  13.160             20.204   \n",
       "4                  14.114             21.543   \n",
       "...                   ...                ...   \n",
       "40093              36.457              0.029   \n",
       "40094              31.586              0.027   \n",
       "40095              31.013              0.028   \n",
       "40096              30.649                NaN   \n",
       "40097              29.731                NaN   \n",
       "\n",
       "                       type_of_stations  population   latitude   longitude  \\\n",
       "0                Urban, Urban, Suburban    246056.0  43.367900   -8.418571   \n",
       "1                Urban, Urban, Suburban    246056.0  43.368033   -8.418233   \n",
       "2      Urban, Urban, Suburban, Suburban    246056.0  43.370375   -8.422900   \n",
       "3      Urban, Urban, Suburban, Suburban    246056.0  43.370375   -8.422900   \n",
       "4      Urban, Urban, Suburban, Suburban    246056.0  43.370375   -8.422900   \n",
       "...                                 ...         ...        ...         ...   \n",
       "40093                              <NA>         NaN  37.337200  126.724100   \n",
       "40094                              <NA>         NaN  37.337200  126.724100   \n",
       "40095                              <NA>         NaN  37.337200  126.724100   \n",
       "40096                              <NA>         NaN  23.012778  113.794444   \n",
       "40097                              <NA>         NaN  23.012778  113.794444   \n",
       "\n",
       "       target_class simplified_station_type  encoded_station_type  \\\n",
       "0                 3         Suburban, Urban                   4.0   \n",
       "1                 4         Suburban, Urban                   4.0   \n",
       "2                 4         Suburban, Urban                   4.0   \n",
       "3                 3         Suburban, Urban                   4.0   \n",
       "4                 4         Suburban, Urban                   4.0   \n",
       "...             ...                     ...                   ...   \n",
       "40093             5                 unknown                   NaN   \n",
       "40094             5                 unknown                   NaN   \n",
       "40095             5                 unknown                   NaN   \n",
       "40096             2                 unknown                   NaN   \n",
       "40097             2                 unknown                   NaN   \n",
       "\n",
       "       encoded_station_type_imputed final_station_type  \n",
       "0                               4.0    Suburban, Urban  \n",
       "1                               4.0    Suburban, Urban  \n",
       "2                               4.0    Suburban, Urban  \n",
       "3                               4.0    Suburban, Urban  \n",
       "4                               4.0    Suburban, Urban  \n",
       "...                             ...                ...  \n",
       "40093                           1.6              Rural  \n",
       "40094                           1.4              Urban  \n",
       "40095                           1.0              Urban  \n",
       "40096                           1.4              Urban  \n",
       "40097                           1.6              Rural  \n",
       "\n",
       "[40002 rows x 16 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_scale_data(df):\n",
    "    # Reset index to ensure it's sequential and clean\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Drop rows with missing values in critical columns\n",
    "    df = df.dropna(subset=['country_name', 'city', 'year', 'population', 'target_class', 'latitude', 'longitude'])\n",
    "\n",
    "    # Ensure 'target_class' is treated as a category\n",
    "    df['target_class'] = df['target_class'].astype('category')\n",
    "\n",
    "    # Convert 'year' to integer\n",
    "    df['year'] = df['year'].astype(int)\n",
    "\n",
    "    columns_to_drop = ['pm10_concentration', 'pm25_concentration', 'no2_concentration', 'type_of_stations',\n",
    "                       'simplified_station_type', 'encoded_station_type', 'encoded_station_type_imputed']\n",
    "\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "    # Define the columns for encoding and scaling\n",
    "    categorical_cols = ['who_region', 'country_name', 'final_station_type']\n",
    "    numeric_cols = ['population', 'latitude', 'longitude']\n",
    "\n",
    "    # Instantiate encoders and scalers\n",
    "    onehot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Pipeline for encoding and scaling\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('onehot', onehot_encoder, categorical_cols),\n",
    "            ('scaler', scaler, numeric_cols)\n",
    "        ],\n",
    "        remainder='passthrough'  # Keep the year and target_class unchanged\n",
    "    )\n",
    "\n",
    "    # Apply transformations\n",
    "    transformed_data = preprocessor.fit_transform(df)\n",
    "\n",
    "    # Get the feature names after one-hot encoding\n",
    "    ohe_feature_names = preprocessor.named_transformers_['onehot'].get_feature_names_out(categorical_cols)\n",
    "\n",
    "    # Construct the final DataFrame\n",
    "    final_columns = list(ohe_feature_names) + numeric_cols + ['city'] + ['year'] + ['target_class']\n",
    "    df_transformed = pd.DataFrame(transformed_data, columns=final_columns)\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = encode_scale_data(data_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>who_region_2_Amr</th>\n",
       "      <th>who_region_3_Sear</th>\n",
       "      <th>who_region_4_Eur</th>\n",
       "      <th>who_region_5_Emr</th>\n",
       "      <th>who_region_6_Wpr</th>\n",
       "      <th>who_region_7_NonMS</th>\n",
       "      <th>country_name_Albania</th>\n",
       "      <th>country_name_Algeria</th>\n",
       "      <th>country_name_Andorra</th>\n",
       "      <th>country_name_Argentina</th>\n",
       "      <th>...</th>\n",
       "      <th>final_station_type_Suburban, Urban</th>\n",
       "      <th>final_station_type_Traffic</th>\n",
       "      <th>final_station_type_Urban</th>\n",
       "      <th>final_station_type_nan</th>\n",
       "      <th>population</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.116974</td>\n",
       "      <td>0.198152</td>\n",
       "      <td>-0.333542</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.116974</td>\n",
       "      <td>0.19816</td>\n",
       "      <td>-0.333536</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.116974</td>\n",
       "      <td>0.198305</td>\n",
       "      <td>-0.333617</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.116974</td>\n",
       "      <td>0.198305</td>\n",
       "      <td>-0.333617</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.116974</td>\n",
       "      <td>0.198305</td>\n",
       "      <td>-0.333617</td>\n",
       "      <td>A Coruna</td>\n",
       "      <td>2017</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  who_region_2_Amr who_region_3_Sear who_region_4_Eur who_region_5_Emr  \\\n",
       "0              0.0               0.0              1.0              0.0   \n",
       "1              0.0               0.0              1.0              0.0   \n",
       "2              0.0               0.0              1.0              0.0   \n",
       "3              0.0               0.0              1.0              0.0   \n",
       "4              0.0               0.0              1.0              0.0   \n",
       "\n",
       "  who_region_6_Wpr who_region_7_NonMS country_name_Albania  \\\n",
       "0              0.0                0.0                  0.0   \n",
       "1              0.0                0.0                  0.0   \n",
       "2              0.0                0.0                  0.0   \n",
       "3              0.0                0.0                  0.0   \n",
       "4              0.0                0.0                  0.0   \n",
       "\n",
       "  country_name_Algeria country_name_Andorra country_name_Argentina  ...  \\\n",
       "0                  0.0                  0.0                    0.0  ...   \n",
       "1                  0.0                  0.0                    0.0  ...   \n",
       "2                  0.0                  0.0                    0.0  ...   \n",
       "3                  0.0                  0.0                    0.0  ...   \n",
       "4                  0.0                  0.0                    0.0  ...   \n",
       "\n",
       "  final_station_type_Suburban, Urban final_station_type_Traffic  \\\n",
       "0                                1.0                        0.0   \n",
       "1                                1.0                        0.0   \n",
       "2                                1.0                        0.0   \n",
       "3                                1.0                        0.0   \n",
       "4                                1.0                        0.0   \n",
       "\n",
       "  final_station_type_Urban final_station_type_nan population  latitude  \\\n",
       "0                      0.0                    0.0  -0.116974  0.198152   \n",
       "1                      0.0                    0.0  -0.116974   0.19816   \n",
       "2                      0.0                    0.0  -0.116974  0.198305   \n",
       "3                      0.0                    0.0  -0.116974  0.198305   \n",
       "4                      0.0                    0.0  -0.116974  0.198305   \n",
       "\n",
       "  longitude      city  year target_class  \n",
       "0 -0.333542  A Coruna  2013            3  \n",
       "1 -0.333536  A Coruna  2014            4  \n",
       "2 -0.333617  A Coruna  2015            4  \n",
       "3 -0.333617  A Coruna  2016            3  \n",
       "4 -0.333617  A Coruna  2017            4  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pycaret.classification import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (30895, 152)\n",
      "Testing set size: (7724, 152)\n"
     ]
    }
   ],
   "source": [
    "final_data = final_data.drop(columns=['city'])\n",
    "# Train split the data\n",
    "train_df, test_df = train_test_split(final_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(f\"Training set size: {train_df.shape}\")\n",
    "print(f\"Testing set size: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c23d0_row9_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c23d0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c23d0_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_c23d0_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c23d0_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_c23d0_row0_col1\" class=\"data row0 col1\" >123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c23d0_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_c23d0_row1_col1\" class=\"data row1 col1\" >target_class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c23d0_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_c23d0_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c23d0_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
       "      <td id=\"T_c23d0_row3_col1\" class=\"data row3 col1\" >1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c23d0_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_c23d0_row4_col1\" class=\"data row4 col1\" >(30895, 152)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c23d0_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_c23d0_row5_col1\" class=\"data row5 col1\" >(30895, 164)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c23d0_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_c23d0_row6_col1\" class=\"data row6 col1\" >(21626, 164)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c23d0_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_c23d0_row7_col1\" class=\"data row7 col1\" >(9269, 164)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c23d0_row8_col0\" class=\"data row8 col0\" >Categorical features</td>\n",
       "      <td id=\"T_c23d0_row8_col1\" class=\"data row8 col1\" >151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c23d0_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_c23d0_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_c23d0_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_c23d0_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_c23d0_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_c23d0_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_c23d0_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_c23d0_row12_col1\" class=\"data row12 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_c23d0_row13_col0\" class=\"data row13 col0\" >Maximum one-hot encoding</td>\n",
       "      <td id=\"T_c23d0_row13_col1\" class=\"data row13 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_c23d0_row14_col0\" class=\"data row14 col0\" >Encoding method</td>\n",
       "      <td id=\"T_c23d0_row14_col1\" class=\"data row14 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_c23d0_row15_col0\" class=\"data row15 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_c23d0_row15_col1\" class=\"data row15 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_c23d0_row16_col0\" class=\"data row16 col0\" >Fold Number</td>\n",
       "      <td id=\"T_c23d0_row16_col1\" class=\"data row16 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_c23d0_row17_col0\" class=\"data row17 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_c23d0_row17_col1\" class=\"data row17 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_c23d0_row18_col0\" class=\"data row18 col0\" >Use GPU</td>\n",
       "      <td id=\"T_c23d0_row18_col1\" class=\"data row18 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_c23d0_row19_col0\" class=\"data row19 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_c23d0_row19_col1\" class=\"data row19 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_c23d0_row20_col0\" class=\"data row20 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_c23d0_row20_col1\" class=\"data row20 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c23d0_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_c23d0_row21_col0\" class=\"data row21 col0\" >USI</td>\n",
       "      <td id=\"T_c23d0_row21_col1\" class=\"data row21 col1\" >87b7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa2a4978f70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3a859 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_3a859_row0_col0, #T_3a859_row0_col2, #T_3a859_row1_col0, #T_3a859_row1_col1, #T_3a859_row1_col3, #T_3a859_row1_col4, #T_3a859_row1_col5, #T_3a859_row1_col6, #T_3a859_row1_col7, #T_3a859_row2_col0, #T_3a859_row2_col1, #T_3a859_row2_col2, #T_3a859_row2_col3, #T_3a859_row2_col4, #T_3a859_row2_col5, #T_3a859_row2_col6, #T_3a859_row2_col7, #T_3a859_row3_col0, #T_3a859_row3_col1, #T_3a859_row3_col2, #T_3a859_row3_col3, #T_3a859_row3_col4, #T_3a859_row3_col5, #T_3a859_row3_col6, #T_3a859_row3_col7, #T_3a859_row4_col0, #T_3a859_row4_col1, #T_3a859_row4_col2, #T_3a859_row4_col3, #T_3a859_row4_col4, #T_3a859_row4_col5, #T_3a859_row4_col6, #T_3a859_row4_col7, #T_3a859_row5_col0, #T_3a859_row5_col1, #T_3a859_row5_col2, #T_3a859_row5_col3, #T_3a859_row5_col4, #T_3a859_row5_col5, #T_3a859_row5_col6, #T_3a859_row5_col7, #T_3a859_row6_col0, #T_3a859_row6_col1, #T_3a859_row6_col2, #T_3a859_row6_col3, #T_3a859_row6_col4, #T_3a859_row6_col5, #T_3a859_row6_col6, #T_3a859_row6_col7, #T_3a859_row7_col0, #T_3a859_row7_col1, #T_3a859_row7_col2, #T_3a859_row7_col3, #T_3a859_row7_col4, #T_3a859_row7_col5, #T_3a859_row7_col6, #T_3a859_row7_col7, #T_3a859_row8_col0, #T_3a859_row8_col1, #T_3a859_row8_col2, #T_3a859_row8_col3, #T_3a859_row8_col4, #T_3a859_row8_col5, #T_3a859_row8_col6, #T_3a859_row8_col7, #T_3a859_row9_col0, #T_3a859_row9_col1, #T_3a859_row9_col2, #T_3a859_row9_col3, #T_3a859_row9_col4, #T_3a859_row9_col5, #T_3a859_row9_col6, #T_3a859_row9_col7, #T_3a859_row10_col0, #T_3a859_row10_col1, #T_3a859_row10_col2, #T_3a859_row10_col3, #T_3a859_row10_col4, #T_3a859_row10_col5, #T_3a859_row10_col6, #T_3a859_row10_col7, #T_3a859_row11_col0, #T_3a859_row11_col1, #T_3a859_row11_col2, #T_3a859_row11_col3, #T_3a859_row11_col4, #T_3a859_row11_col5, #T_3a859_row11_col6, #T_3a859_row11_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_3a859_row0_col1, #T_3a859_row0_col3, #T_3a859_row0_col4, #T_3a859_row0_col5, #T_3a859_row0_col6, #T_3a859_row0_col7, #T_3a859_row1_col2 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_3a859_row0_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_3a859_row1_col8, #T_3a859_row2_col8, #T_3a859_row3_col8, #T_3a859_row4_col8, #T_3a859_row5_col8, #T_3a859_row6_col8, #T_3a859_row7_col8, #T_3a859_row8_col8, #T_3a859_row9_col8, #T_3a859_row10_col8, #T_3a859_row11_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3a859\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3a859_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_3a859_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_3a859_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_3a859_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_3a859_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_3a859_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_3a859_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_3a859_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_3a859_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row0\" class=\"row_heading level0 row0\" >dt</th>\n",
       "      <td id=\"T_3a859_row0_col0\" class=\"data row0 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_3a859_row0_col1\" class=\"data row0 col1\" >0.6690</td>\n",
       "      <td id=\"T_3a859_row0_col2\" class=\"data row0 col2\" >0.7864</td>\n",
       "      <td id=\"T_3a859_row0_col3\" class=\"data row0 col3\" >0.6690</td>\n",
       "      <td id=\"T_3a859_row0_col4\" class=\"data row0 col4\" >0.6706</td>\n",
       "      <td id=\"T_3a859_row0_col5\" class=\"data row0 col5\" >0.6695</td>\n",
       "      <td id=\"T_3a859_row0_col6\" class=\"data row0 col6\" >0.5685</td>\n",
       "      <td id=\"T_3a859_row0_col7\" class=\"data row0 col7\" >0.5686</td>\n",
       "      <td id=\"T_3a859_row0_col8\" class=\"data row0 col8\" >0.4310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row1\" class=\"row_heading level0 row1\" >rf</th>\n",
       "      <td id=\"T_3a859_row1_col0\" class=\"data row1 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_3a859_row1_col1\" class=\"data row1 col1\" >0.6292</td>\n",
       "      <td id=\"T_3a859_row1_col2\" class=\"data row1 col2\" >0.8806</td>\n",
       "      <td id=\"T_3a859_row1_col3\" class=\"data row1 col3\" >0.6292</td>\n",
       "      <td id=\"T_3a859_row1_col4\" class=\"data row1 col4\" >0.6217</td>\n",
       "      <td id=\"T_3a859_row1_col5\" class=\"data row1 col5\" >0.6231</td>\n",
       "      <td id=\"T_3a859_row1_col6\" class=\"data row1 col6\" >0.5113</td>\n",
       "      <td id=\"T_3a859_row1_col7\" class=\"data row1 col7\" >0.5125</td>\n",
       "      <td id=\"T_3a859_row1_col8\" class=\"data row1 col8\" >1.2820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row2\" class=\"row_heading level0 row2\" >gbc</th>\n",
       "      <td id=\"T_3a859_row2_col0\" class=\"data row2 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_3a859_row2_col1\" class=\"data row2 col1\" >0.5946</td>\n",
       "      <td id=\"T_3a859_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row2_col3\" class=\"data row2 col3\" >0.5946</td>\n",
       "      <td id=\"T_3a859_row2_col4\" class=\"data row2 col4\" >0.5847</td>\n",
       "      <td id=\"T_3a859_row2_col5\" class=\"data row2 col5\" >0.5717</td>\n",
       "      <td id=\"T_3a859_row2_col6\" class=\"data row2 col6\" >0.4548</td>\n",
       "      <td id=\"T_3a859_row2_col7\" class=\"data row2 col7\" >0.4642</td>\n",
       "      <td id=\"T_3a859_row2_col8\" class=\"data row2 col8\" >4.0390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row3\" class=\"row_heading level0 row3\" >knn</th>\n",
       "      <td id=\"T_3a859_row3_col0\" class=\"data row3 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_3a859_row3_col1\" class=\"data row3 col1\" >0.5636</td>\n",
       "      <td id=\"T_3a859_row3_col2\" class=\"data row3 col2\" >0.8113</td>\n",
       "      <td id=\"T_3a859_row3_col3\" class=\"data row3 col3\" >0.5636</td>\n",
       "      <td id=\"T_3a859_row3_col4\" class=\"data row3 col4\" >0.5570</td>\n",
       "      <td id=\"T_3a859_row3_col5\" class=\"data row3 col5\" >0.5585</td>\n",
       "      <td id=\"T_3a859_row3_col6\" class=\"data row3 col6\" >0.4255</td>\n",
       "      <td id=\"T_3a859_row3_col7\" class=\"data row3 col7\" >0.4263</td>\n",
       "      <td id=\"T_3a859_row3_col8\" class=\"data row3 col8\" >0.9590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row4\" class=\"row_heading level0 row4\" >lr</th>\n",
       "      <td id=\"T_3a859_row4_col0\" class=\"data row4 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_3a859_row4_col1\" class=\"data row4 col1\" >0.5389</td>\n",
       "      <td id=\"T_3a859_row4_col2\" class=\"data row4 col2\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row4_col3\" class=\"data row4 col3\" >0.5389</td>\n",
       "      <td id=\"T_3a859_row4_col4\" class=\"data row4 col4\" >0.5160</td>\n",
       "      <td id=\"T_3a859_row4_col5\" class=\"data row4 col5\" >0.5031</td>\n",
       "      <td id=\"T_3a859_row4_col6\" class=\"data row4 col6\" >0.3766</td>\n",
       "      <td id=\"T_3a859_row4_col7\" class=\"data row4 col7\" >0.3878</td>\n",
       "      <td id=\"T_3a859_row4_col8\" class=\"data row4 col8\" >1.4510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row5\" class=\"row_heading level0 row5\" >lda</th>\n",
       "      <td id=\"T_3a859_row5_col0\" class=\"data row5 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_3a859_row5_col1\" class=\"data row5 col1\" >0.5178</td>\n",
       "      <td id=\"T_3a859_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row5_col3\" class=\"data row5 col3\" >0.5178</td>\n",
       "      <td id=\"T_3a859_row5_col4\" class=\"data row5 col4\" >0.5061</td>\n",
       "      <td id=\"T_3a859_row5_col5\" class=\"data row5 col5\" >0.5029</td>\n",
       "      <td id=\"T_3a859_row5_col6\" class=\"data row5 col6\" >0.3571</td>\n",
       "      <td id=\"T_3a859_row5_col7\" class=\"data row5 col7\" >0.3622</td>\n",
       "      <td id=\"T_3a859_row5_col8\" class=\"data row5 col8\" >0.6380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row6\" class=\"row_heading level0 row6\" >ridge</th>\n",
       "      <td id=\"T_3a859_row6_col0\" class=\"data row6 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_3a859_row6_col1\" class=\"data row6 col1\" >0.5229</td>\n",
       "      <td id=\"T_3a859_row6_col2\" class=\"data row6 col2\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row6_col3\" class=\"data row6 col3\" >0.5229</td>\n",
       "      <td id=\"T_3a859_row6_col4\" class=\"data row6 col4\" >0.5022</td>\n",
       "      <td id=\"T_3a859_row6_col5\" class=\"data row6 col5\" >0.4808</td>\n",
       "      <td id=\"T_3a859_row6_col6\" class=\"data row6 col6\" >0.3522</td>\n",
       "      <td id=\"T_3a859_row6_col7\" class=\"data row6 col7\" >0.3649</td>\n",
       "      <td id=\"T_3a859_row6_col8\" class=\"data row6 col8\" >0.4480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row7\" class=\"row_heading level0 row7\" >svm</th>\n",
       "      <td id=\"T_3a859_row7_col0\" class=\"data row7 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_3a859_row7_col1\" class=\"data row7 col1\" >0.5025</td>\n",
       "      <td id=\"T_3a859_row7_col2\" class=\"data row7 col2\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row7_col3\" class=\"data row7 col3\" >0.5025</td>\n",
       "      <td id=\"T_3a859_row7_col4\" class=\"data row7 col4\" >0.5181</td>\n",
       "      <td id=\"T_3a859_row7_col5\" class=\"data row7 col5\" >0.4668</td>\n",
       "      <td id=\"T_3a859_row7_col6\" class=\"data row7 col6\" >0.3286</td>\n",
       "      <td id=\"T_3a859_row7_col7\" class=\"data row7 col7\" >0.3469</td>\n",
       "      <td id=\"T_3a859_row7_col8\" class=\"data row7 col8\" >0.7040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row8\" class=\"row_heading level0 row8\" >ada</th>\n",
       "      <td id=\"T_3a859_row8_col0\" class=\"data row8 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_3a859_row8_col1\" class=\"data row8 col1\" >0.4730</td>\n",
       "      <td id=\"T_3a859_row8_col2\" class=\"data row8 col2\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row8_col3\" class=\"data row8 col3\" >0.4730</td>\n",
       "      <td id=\"T_3a859_row8_col4\" class=\"data row8 col4\" >0.4653</td>\n",
       "      <td id=\"T_3a859_row8_col5\" class=\"data row8 col5\" >0.4406</td>\n",
       "      <td id=\"T_3a859_row8_col6\" class=\"data row8 col6\" >0.2890</td>\n",
       "      <td id=\"T_3a859_row8_col7\" class=\"data row8 col7\" >0.2997</td>\n",
       "      <td id=\"T_3a859_row8_col8\" class=\"data row8 col8\" >0.7810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row9\" class=\"row_heading level0 row9\" >dummy</th>\n",
       "      <td id=\"T_3a859_row9_col0\" class=\"data row9 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_3a859_row9_col1\" class=\"data row9 col1\" >0.3410</td>\n",
       "      <td id=\"T_3a859_row9_col2\" class=\"data row9 col2\" >0.5000</td>\n",
       "      <td id=\"T_3a859_row9_col3\" class=\"data row9 col3\" >0.3410</td>\n",
       "      <td id=\"T_3a859_row9_col4\" class=\"data row9 col4\" >0.1163</td>\n",
       "      <td id=\"T_3a859_row9_col5\" class=\"data row9 col5\" >0.1734</td>\n",
       "      <td id=\"T_3a859_row9_col6\" class=\"data row9 col6\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row9_col7\" class=\"data row9 col7\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row9_col8\" class=\"data row9 col8\" >0.4540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row10\" class=\"row_heading level0 row10\" >nb</th>\n",
       "      <td id=\"T_3a859_row10_col0\" class=\"data row10 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_3a859_row10_col1\" class=\"data row10 col1\" >0.2340</td>\n",
       "      <td id=\"T_3a859_row10_col2\" class=\"data row10 col2\" >0.6853</td>\n",
       "      <td id=\"T_3a859_row10_col3\" class=\"data row10 col3\" >0.2340</td>\n",
       "      <td id=\"T_3a859_row10_col4\" class=\"data row10 col4\" >0.5871</td>\n",
       "      <td id=\"T_3a859_row10_col5\" class=\"data row10 col5\" >0.1727</td>\n",
       "      <td id=\"T_3a859_row10_col6\" class=\"data row10 col6\" >0.1277</td>\n",
       "      <td id=\"T_3a859_row10_col7\" class=\"data row10 col7\" >0.1555</td>\n",
       "      <td id=\"T_3a859_row10_col8\" class=\"data row10 col8\" >0.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3a859_level0_row11\" class=\"row_heading level0 row11\" >qda</th>\n",
       "      <td id=\"T_3a859_row11_col0\" class=\"data row11 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_3a859_row11_col1\" class=\"data row11 col1\" >0.1686</td>\n",
       "      <td id=\"T_3a859_row11_col2\" class=\"data row11 col2\" >0.0000</td>\n",
       "      <td id=\"T_3a859_row11_col3\" class=\"data row11 col3\" >0.1686</td>\n",
       "      <td id=\"T_3a859_row11_col4\" class=\"data row11 col4\" >0.5909</td>\n",
       "      <td id=\"T_3a859_row11_col5\" class=\"data row11 col5\" >0.1391</td>\n",
       "      <td id=\"T_3a859_row11_col6\" class=\"data row11 col6\" >0.0832</td>\n",
       "      <td id=\"T_3a859_row11_col7\" class=\"data row11 col7\" >0.1048</td>\n",
       "      <td id=\"T_3a859_row11_col8\" class=\"data row11 col8\" >1.0070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa2a4cace20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_samples_leaf=1,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       monotonic_cst=None, random_state=123, splitter='best')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e5a6a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e5a6a_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_e5a6a_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_e5a6a_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_e5a6a_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_e5a6a_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_e5a6a_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_e5a6a_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_e5a6a_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e5a6a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e5a6a_row0_col0\" class=\"data row0 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_e5a6a_row0_col1\" class=\"data row0 col1\" >0.6915</td>\n",
       "      <td id=\"T_e5a6a_row0_col2\" class=\"data row0 col2\" >0.8002</td>\n",
       "      <td id=\"T_e5a6a_row0_col3\" class=\"data row0 col3\" >0.6915</td>\n",
       "      <td id=\"T_e5a6a_row0_col4\" class=\"data row0 col4\" >0.6921</td>\n",
       "      <td id=\"T_e5a6a_row0_col5\" class=\"data row0 col5\" >0.6917</td>\n",
       "      <td id=\"T_e5a6a_row0_col6\" class=\"data row0 col6\" >0.5971</td>\n",
       "      <td id=\"T_e5a6a_row0_col7\" class=\"data row0 col7\" >0.5971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa2a4cac520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      who_region_2_Amr who_region_3_Sear who_region_4_Eur who_region_5_Emr  \\\n",
      "29073              0.0               0.0              1.0              0.0   \n",
      "2323               0.0               0.0              1.0              0.0   \n",
      "8879               0.0               1.0              0.0              0.0   \n",
      "5503               0.0               0.0              1.0              0.0   \n",
      "14326              1.0               0.0              0.0              0.0   \n",
      "...                ...               ...              ...              ...   \n",
      "13088              0.0               0.0              1.0              0.0   \n",
      "35184              0.0               0.0              1.0              0.0   \n",
      "16780              0.0               0.0              1.0              0.0   \n",
      "19791              0.0               0.0              1.0              0.0   \n",
      "37123              0.0               0.0              0.0              0.0   \n",
      "\n",
      "      who_region_6_Wpr who_region_7_NonMS country_name_Albania  \\\n",
      "29073              0.0                0.0                  0.0   \n",
      "2323               0.0                0.0                  0.0   \n",
      "8879               0.0                0.0                  0.0   \n",
      "5503               0.0                0.0                  0.0   \n",
      "14326              0.0                0.0                  0.0   \n",
      "...                ...                ...                  ...   \n",
      "13088              0.0                0.0                  0.0   \n",
      "35184              0.0                0.0                  0.0   \n",
      "16780              0.0                0.0                  0.0   \n",
      "19791              0.0                0.0                  0.0   \n",
      "37123              1.0                0.0                  0.0   \n",
      "\n",
      "      country_name_Algeria country_name_Andorra country_name_Argentina  ...  \\\n",
      "29073                  0.0                  0.0                    0.0  ...   \n",
      "2323                   0.0                  0.0                    0.0  ...   \n",
      "8879                   0.0                  0.0                    0.0  ...   \n",
      "5503                   0.0                  0.0                    0.0  ...   \n",
      "14326                  0.0                  0.0                    0.0  ...   \n",
      "...                    ...                  ...                    ...  ...   \n",
      "13088                  0.0                  0.0                    0.0  ...   \n",
      "35184                  0.0                  0.0                    0.0  ...   \n",
      "16780                  0.0                  0.0                    0.0  ...   \n",
      "19791                  0.0                  0.0                    0.0  ...   \n",
      "37123                  0.0                  0.0                    0.0  ...   \n",
      "\n",
      "      final_station_type_Traffic final_station_type_Urban  \\\n",
      "29073                        0.0                      0.0   \n",
      "2323                         0.0                      0.0   \n",
      "8879                         0.0                      1.0   \n",
      "5503                         0.0                      1.0   \n",
      "14326                        0.0                      0.0   \n",
      "...                          ...                      ...   \n",
      "13088                        0.0                      1.0   \n",
      "35184                        0.0                      0.0   \n",
      "16780                        0.0                      0.0   \n",
      "19791                        0.0                      0.0   \n",
      "37123                        0.0                      0.0   \n",
      "\n",
      "      final_station_type_nan population  latitude longitude  year  \\\n",
      "29073                    0.0  -0.177377  0.472861  0.038504  2013   \n",
      "2323                     0.0  -0.247259  0.025028 -0.244199  2017   \n",
      "8879                     0.0   0.545793 -1.006695  1.311646  2012   \n",
      "5503                     0.0   0.023631 -0.057619 -0.029600  2019   \n",
      "14326                    0.0   3.623791 -0.648372 -1.839095  2012   \n",
      "...                      ...        ...       ...       ...   ...   \n",
      "13088                    0.0  -0.262128  0.122549  0.260939  2014   \n",
      "35184                    0.0  -0.266795  0.128089 -0.028300  2020   \n",
      "16780                    0.0  -0.266653  0.498038  0.075969  2021   \n",
      "19791                    0.0  -0.266563  0.342382  0.138888  2014   \n",
      "37123                    0.0   0.021821 -0.672502  1.837407  2019   \n",
      "\n",
      "      target_class prediction_label prediction_score  \n",
      "29073            3                3              1.0  \n",
      "2323             1                1              1.0  \n",
      "8879             6                6              1.0  \n",
      "5503             4                4              1.0  \n",
      "14326            4                3              1.0  \n",
      "...            ...              ...              ...  \n",
      "13088            4                4              1.0  \n",
      "35184            1                1              1.0  \n",
      "16780            1                1              1.0  \n",
      "19791            3                3              1.0  \n",
      "37123            3                2              1.0  \n",
      "\n",
      "[7724 rows x 154 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42b50e2c4004cb0b9720d16fde32399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup PyCaret with the training data\n",
    "\n",
    "import logging\n",
    "#logging.getLogger('pycaret').setLevel(logging.ERROR)\n",
    "\n",
    "clf = setup(data=train_df, target='target_class', session_id=123, verbose=True)\n",
    "\n",
    "# Compare models to find the best one\n",
    "best_model = compare_models(exclude=['lightgbm', 'et'], sort='F1')\n",
    "\n",
    "# Print details of the best model\n",
    "print(best_model)\n",
    "# Tune the model\n",
    "#tuned_rf = tune_model(best_model)\n",
    "#print(tuned_rf)\n",
    "\n",
    "# Finalize the model to make predictions on the test set\n",
    "final_model = finalize_model(best_model)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = predict_model(final_model, data=test_df)\n",
    "print(predictions)\n",
    "evaluate_model(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air-pollution-levels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
